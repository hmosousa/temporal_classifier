model:
    model_name:  HuggingFaceTB/SmolLM2-135M
    trust_remote_code: True

data:
    dataset_name: temporal_questions
    batch_size: 2
    max_seq_length: 1024
    augment: False
    pad_to_max_length: False  # To pad each batch at runtime
    shuffle_train_dataset: True
    shuffle_seed: 42

trainer:
    run_name: debug
    eval_strategy: epoch
    per_device_train_batch_size: 2
    per_device_eval_batch_size: 2
    num_train_epochs: 5
    gradient_accumulation_steps: 1
    learning_rate: 1e-3
    max_grad_norm: 1.0
    lr_scheduler_type: reduce_lr_on_plateau
    lr_scheduler_kwargs:
        factor: 0.5
        patience: 1
    logging_dir: logs
    logging_steps: 1
    save_total_limit: 2
    save_strategy: epoch
    load_best_model_at_end: True
    metric_for_best_model: eval_loss
    greater_is_better: False
    seed: 42
    bf16: True
    push_to_hub: False
    hub_model_id: hugosousa/debug
    hub_strategy: every_save
    hub_token: HF_TOKEN
    do_train: True
    do_eval: True
    skip_memory_metrics: True
    use_liger_kernel: False
    label_smoothing_factor: 0.0
    torch_compile: False
